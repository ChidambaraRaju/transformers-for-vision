{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Efficient Image Transformer (DeiT) from Scratch\n",
    "\n",
    "This notebook implements **DeiT (Data-efficient Image Transformers)**. While standard Vision Transformers (ViT) usually require massive datasets (like JFT-300M) to outperform CNNs, DeiT introduces a training strategy that allows Transformers to train effectively on smaller datasets (like ImageNet or even MNIST) by leaning on a strong \"Teacher\" network.\n",
    "\n",
    "### Key Concepts:\n",
    "1.  **Teacher-Student Distillation:** We use a pre-trained strong CNN (ResNet50) as a \"Teacher\".\n",
    "2.  **The Distillation Token:** Unlike a standard ViT which has one class token `[CLS]`, DeiT adds a second token `[DIST]`. \n",
    "    * The `[CLS]` token learns from the true labels (Ground Truth).\n",
    "    * The `[DIST]` token learns to mimic the Teacher's predictions.\n",
    "3.  **Hard vs Soft Distillation:** We will implement **Soft Distillation** using KL-Divergence, minimizing the difference between the Student's and Teacher's output distributions.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms, datasets, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 28\n",
    "PATCH_SIZE = 7\n",
    "CHANNELS = 3          # We repeat MNIST channels to 3 to match ResNet input expectations\n",
    "CLASSES = 10\n",
    "\n",
    "# Model Params\n",
    "EMBED_DIM = 32\n",
    "ATTENTION_HEADS = 4\n",
    "TRANSFORMER_LAYERS = 4\n",
    "\n",
    "# Training Params\n",
    "EPOCHS_STUDENT = 5\n",
    "LR_STUDENT = 0.001\n",
    "LR_TEACHER = 0.001\n",
    "\n",
    "# Distillation Params\n",
    "TEMPERATURE = 2       # Softens the probability distributions (higher = softer)\n",
    "ALPHA = 0.5           # Weighting: 0.5 * Student_Loss + 0.5 * Distillation_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We use MNIST for this demonstration. \n",
    "\n",
    "**Note:** ResNet (our teacher) expects 3-channel images. Since MNIST is grayscale (1 channel), we use a lambda transform to repeat the channel 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: t.repeat(3, 1, 1)), # Convert 1x28x28 -> 3x28x28\n",
    "])\n",
    "\n",
    "train_full = datasets.MNIST('./data', train=True, download=True, transform=tfm)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=tfm)\n",
    "\n",
    "# Use a subset for faster demonstration if needed, currently using 100% of data\n",
    "n = int(1.0 * len(train_full))\n",
    "subset_idx = np.random.permutation(len(train_full))[:n]\n",
    "train_dataset = Subset(train_full, subset_idx)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(test_dataset, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Teacher Model (ResNet50)\n",
    "\n",
    "In Knowledge Distillation, the Teacher must be a strong model. We use a **ResNet50** pre-trained on ImageNet.\n",
    "\n",
    "Since ResNet outputs 1000 classes (ImageNet), we replace the final layer to output 10 classes (MNIST). We then fine-tune it briefly so it actually knows how to classify digits. The Student will later try to \"clone\" this knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained ResNet50\n",
    "teacher = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Modify the final layer for MNIST (10 classes)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, CLASSES)\n",
    "teacher.to(device)\n",
    "\n",
    "# Freeze all layers except the last classification layer for efficiency\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in teacher.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Teacher model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the Teacher\n",
    "optimizer_teacher = torch.optim.Adam(teacher.fc.parameters(), lr=LR_TEACHER)\n",
    "criterion_teacher = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS_TEACHER = 3  # A few epochs are enough for the teacher to learn MNIST\n",
    "\n",
    "print(\"Start Teacher Fine-tuning...\")\n",
    "for epoch in range(EPOCHS_TEACHER):\n",
    "    teacher.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dl:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_teacher.zero_grad()\n",
    "        outputs = teacher(inputs)\n",
    "        loss = criterion_teacher(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_teacher.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"  Teacher Epoch {epoch+1}, Avg Loss: {running_loss / len(train_dl):.4f}\")\n",
    "\n",
    "# Evaluate Teacher\n",
    "teacher.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_dl:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = teacher(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Teacher Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Student Model (DeiT)\n",
    "\n",
    "This is the standard Vision Transformer architecture with one crucial modification:\n",
    "\n",
    "**The Input Sequence:**\n",
    "Instead of `[Class Token, Patch 1, Patch 2, ...]`, DeiT uses:\n",
    "`[Class Token, Distillation Token, Patch 1, Patch 2, ...]`\n",
    "\n",
    "Both tokens interact with the image patches via Self-Attention, but they feed into separate heads at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Splits image into patches and embeds them.\"\"\"\n",
    "    def __init__(self, channels=CHANNELS, embed_dim=EMBED_DIM, patch_size=PATCH_SIZE, img_size=IMG_SIZE):\n",
    "        super().__init__()\n",
    "        # We use a Convolution to handle splitting and embedding simultaneously\n",
    "        self.proj = nn.Conv2d(in_channels=channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Channels, Height, Width)\n",
    "        x = self.proj(x)      # (Batch, Embed_Dim, H', W')\n",
    "        x = x.flatten(2)      # (Batch, Embed_Dim, N_Patches)\n",
    "        x = x.transpose(1, 2) # (Batch, N_Patches, Embed_Dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeiT(nn.Module):\n",
    "    def __init__(self, embed_dim=EMBED_DIM, attention_heads=ATTENTION_HEADS, layers=TRANSFORMER_LAYERS, classes=CLASSES):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed()\n",
    "        \n",
    "        # 1. Class Token (Standard ViT)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # 2. Distillation Token (DeiT Specific)\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        \n",
    "        # Position Embedding: N_Patches + 2 tokens (CLS + DIST)\n",
    "        self.n_patches = self.patch_embed.n_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches + 2, embed_dim))\n",
    "        \n",
    "        # Transformer Encoder Blocks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=attention_heads, batch_first=True)\n",
    "        self.transformer_blocks = nn.TransformerEncoder(encoder_layer, num_layers=layers)\n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Two separate heads\n",
    "        self.head_cls = nn.Linear(embed_dim, classes)     # For Ground Truth\n",
    "        self.head_dist = nn.Linear(embed_dim, classes)    # For Teacher Prediction\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Embed Patches\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Expand Tokens to batch size\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "        \n",
    "        # Concatenate: [CLS, DIST, Patches]\n",
    "        x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        \n",
    "        # Add Position Embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer Pass\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        # Extract specific tokens\n",
    "        # Index 0 is CLS, Index 1 is DIST\n",
    "        out_cls = x[:, 0]\n",
    "        out_dist = x[:, 1]\n",
    "        \n",
    "        # Pass through respective heads\n",
    "        logits_cls = self.head_cls(out_cls)\n",
    "        logits_dist = self.head_dist(out_dist)\n",
    "        \n",
    "        return logits_cls, logits_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distillation Loss Function\n",
    "\n",
    "The total loss is a combination of two losses:\n",
    "1.  **Student Loss (CrossEntropy):** Does the `[CLS]` token predict the correct digit?\n",
    "2.  **Distillation Loss (KL Divergence):** Does the `[DIST]` token output the same probability distribution as the Teacher?\n",
    "\n",
    "$$ Loss = \\alpha \\cdot \\text{KL}(Student_{dist}, Teacher) + (1-\\alpha) \\cdot \\text{CE}(Student_{cls}, Label) $$\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(student_cls_logits, student_dist_logits, teacher_logits, labels, alpha=ALPHA, temperature=TEMPERATURE):\n",
    "    # 1. Standard Classification Loss (Ground Truth)\n",
    "    loss_ce = F.cross_entropy(student_cls_logits, labels)\n",
    "    \n",
    "    # 2. Distillation Loss (Teacher Knowledge)\n",
    "    # We soften the logits by dividing by temperature to reveal \"dark knowledge\" (relationships between classes)\n",
    "    distillation = F.kl_div(\n",
    "        F.log_softmax(student_dist_logits / temperature, dim=1),\n",
    "        F.softmax(teacher_logits / temperature, dim=1),\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    # Combine losses\n",
    "    total_loss = (alpha * distillation) + ((1 - alpha) * loss_ce)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training the Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = DeiT().to(device)\n",
    "optimizer_student = torch.optim.Adam(student.parameters(), lr=LR_STUDENT)\n",
    "\n",
    "print(\"Training Student (DeiT)...\")\n",
    "for epoch in range(EPOCHS_STUDENT):\n",
    "    student.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_dl:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Get Teacher Predictions (No Gradient needed for Teacher)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(inputs)\n",
    "            \n",
    "        # 2. Get Student Predictions\n",
    "        cls_logits, dist_logits = student(inputs)\n",
    "        \n",
    "        # 3. Calculate Loss\n",
    "        loss = distillation_loss(cls_logits, dist_logits, teacher_logits, labels)\n",
    "        \n",
    "        # 4. Backprop\n",
    "        optimizer_student.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_student.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_STUDENT}, Loss: {running_loss/len(train_dl):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "During inference, DeiT typically averages the predictions of the `[CLS]` head and the `[DIST]` head for maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_dl:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        cls_logits, dist_logits = student(inputs)\n",
    "        \n",
    "        # Inference Strategy: Average both heads\n",
    "        avg_logits = (cls_logits + dist_logits) / 2\n",
    "        predictions = avg_logits.argmax(dim=1)\n",
    "        \n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Save a few for visualization\n",
    "        if len(samples) < 5:\n",
    "            samples.append((inputs.cpu(), predictions.cpu(), labels.cpu()))\n",
    "\n",
    "acc = 100 * correct / total\n",
    "print(f\"\\nStudent (DeiT) Test Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axs = plt.subplots(1, len(samples), figsize=(15, 3))\n",
    "for i, (img, pred, true) in enumerate(samples):\n",
    "    # Convert Tensor (3, 28, 28) back to Numpy (28, 28, 3)\n",
    "    img_np = img[0].permute(1, 2, 0).numpy()\n",
    "    \n",
    "    axs[i].imshow(img_np, cmap='gray' if CHANNELS==1 else None)\n",
    "    axs[i].set_title(f\"Pred: {pred[0].item()} | True: {true[0].item()}\")\n",
    "    axs[i].axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}