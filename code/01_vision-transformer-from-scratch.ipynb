{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Implementation from Scratch\n",
    "\n",
    "This notebook provides a step-by-step implementation of the **Vision Transformer (ViT)** architecture using PyTorch, applied to the MNIST dataset. \n",
    "\n",
    "Unlike Convolutional Neural Networks (CNNs) that process pixels, ViT treats an image as a sequence of patches, similar to how Transformers in NLP treat sentences as sequences of words.\n",
    "\n",
    "### Key Architecture Steps:\n",
    "1.  **Patch Embedding:** Split the image into fixed-size patches and flatten them.\n",
    "2.  **Position Embedding:** Add learnable position vectors so the model knows the order of patches.\n",
    "3.  **Transformer Encoder:** Process the sequence using Self-Attention and MLPs.\n",
    "4.  **Classification Head:** Use a special \"Class Token\" to predict the final digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set device configuration (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We use the **MNIST** dataset. We strictly convert the images to Tensors. No complex augmentation is used here to keep the focus on the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation: Convert PIL image to Tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load MNIST Dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters\n",
    "Here we define the structural parameters of the ViT. \n",
    "\n",
    "* **Patch Size (7):** We split the 28x28 image into 7x7 squares. This results in $4 \\times 4 = 16$ total patches.\n",
    "* **Embedding Dim (64):** Each patch will be projected into a vector of size 64.\n",
    "* **Attention Heads (4):** The Multi-Head Attention mechanism will split the 64 dimensions into 4 heads of 16 dimensions each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_classes = 10          # MNIST has digits 0-9\n",
    "batch_size = 64\n",
    "num_channels = 1          # Grayscale images\n",
    "img_size = 28             # MNIST image size\n",
    "patch_size = 7            # Size of each patch (must be a divisor of img_size)\n",
    "\n",
    "# Calculated parameters\n",
    "num_patches = (img_size // patch_size) ** 2  # Total patches: (28/7)^2 = 16\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 64        # Dimension of the linear projection of patches\n",
    "attention_heads = 4       # Number of heads in Multi-Head Attention\n",
    "transformer_blocks = 4    # Number of Transformer Encoder layers\n",
    "mlp_hidden_nodes = 128    # Hidden size in the MLP feed-forward network\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patch Embedding Layer\n",
    "\n",
    "**The Logic:** Instead of manually cropping the image into squares, we can use a **Convolutional Layer** (`nn.Conv2d`). \n",
    "\n",
    "If we set `kernel_size` and `stride` both equal to the `patch_size`, the convolution operation naturally hops over the image in non-overlapping grids, effectively creating the patches and projecting them to `embedding_dim` in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Conv2d usage as a Patch Embedder:\n",
    "        # Input: (Batch, 1, 28, 28) -> Output: (Batch, 64, 4, 4)\n",
    "        self.patch_embed = nn.Conv2d(num_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, 1, 28, 28)\n",
    "        \n",
    "        x = self.patch_embed(x) \n",
    "        # x shape: (Batch, 64, 4, 4)\n",
    "        # 64 is the embedding_dim. 4x4 comes from 28/7 = 4.\n",
    "        \n",
    "        x = x.flatten(2) \n",
    "        # Flatten the spatial dimensions (4, 4) into a single sequence dimension (16)\n",
    "        # x shape: (Batch, 64, 16)\n",
    "\n",
    "        x = x.transpose(1, 2) \n",
    "        # Transformers expect sequence length first (or second if batch_first=True)\n",
    "        # x shape: (Batch, 16, 64) -> (Batch, Num_Patches, Embedding_Dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Encoder Block\n",
    "\n",
    "A standard Transformer Encoder block consists of:\n",
    "1.  **Layer Normalization**\n",
    "2.  **Multi-Head Self Attention (MSA)**\n",
    "3.  **Residual Connection** (Add)\n",
    "4.  **Layer Normalization**\n",
    "5.  **Multi-Layer Perceptron (MLP)**\n",
    "6.  **Residual Connection** (Add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Normalization layers\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.multihead_attention = nn.MultiheadAttention(embedding_dim, attention_heads, batch_first=True)\n",
    "        \n",
    "        # Feed Forward Network (MLP)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_hidden_nodes),\n",
    "            nn.GELU(), # GELU is often used in Transformers instead of ReLU\n",
    "            nn.Linear(mlp_hidden_nodes, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1: Attention + Residual\n",
    "        residual1 = x\n",
    "        x = self.layer_norm1(x)\n",
    "        # In self-attention, Query, Key, and Value are all the same input 'x'\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)\n",
    "        x = attn_output + residual1\n",
    "\n",
    "        # Block 2: MLP + Residual\n",
    "        residual2 = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + residual2\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Classification Head (MLP Head)\n",
    "This is the final layer that takes the representation of the **Class Token** and projects it to the 10 output classes (digits 0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vision Transformer (Full Assembly)\n",
    "\n",
    "**Crucial Concepts:**\n",
    "1.  **CLS Token:** Since a Transformer outputs a sequence (one vector per patch), which one do we use for classification? We prepend a special learnable vector (CLS token) to the start of the sequence. We ignore the patch outputs and only use this CLS token output for prediction.\n",
    "2.  **Position Embedding:** Transformers have no sense of \"up/down\" or \"left/right\". We add a learnable position vector to every patch embedding so the model can learn spatial structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "        \n",
    "        # Learnable Class Token (1, 1, embedding_dim)\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, embedding_dim))\n",
    "        \n",
    "        # Learnable Position Embedding (1, num_patches + 1, embedding_dim)\n",
    "        # +1 because we are adding the CLS token to the sequence\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embedding_dim))\n",
    "        \n",
    "        # Stack multiple Transformer Encoder blocks\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerEncoder() for _ in range(transformer_blocks)])\n",
    "        \n",
    "        self.mlp_head = MLPHead()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Embed Patches\n",
    "        x = self.patch_embedding(x) # (Batch, 16, 64)\n",
    "        \n",
    "        # 2. Add CLS Token\n",
    "        batch_size = x.size(0)\n",
    "        class_token = self.cls_token.expand(batch_size, -1, -1) # Expand to match batch size\n",
    "        x = torch.cat([class_token, x], dim=1) # (Batch, 17, 64)\n",
    "        \n",
    "        # 3. Add Position Embedding\n",
    "        x = x + self.position_embedding\n",
    "        \n",
    "        # 4. Pass through Transformer Blocks\n",
    "        x = self.transformer_blocks(x)\n",
    "        \n",
    "        # 5. Extract only the CLS token (index 0) for classification\n",
    "        x = x[:, 0] # (Batch, 64)\n",
    "        \n",
    "        # 6. Final Classification\n",
    "        x = self.mlp_head(x) # (Batch, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model, Optimizer, and Loss Function\n",
    "model = VisionTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_epoch = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct_epoch += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            accuracy = 100 * correct_epoch / total_samples\n",
    "            print(f\"  Batch {batch_idx+1}: Loss = {loss.item():.4f}, Running Acc = {accuracy:.2f}%\")\n",
    "    \n",
    "    epoch_acc = 100 * correct_epoch / total_samples\n",
    "    print(f\"Epoch {epoch+1} Summary: Avg Loss = {total_loss/len(train_loader):.4f}, Accuracy = {epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_acc = 100 * correct / total\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
