# Transformers For Vision

A structured learning and implementation repository covering modern vision transformers and multimodal AI, including handwritten notes, experiments, and end-to-end projects from ViT to multimodal LLMs.

---

## ğŸ“Œ About This Repository

This repository documents my hands-on learning journey through **Transformers for Vision** from Vizuara.  
It combines **conceptual understanding** with **practical implementation**, focusing on how transformer-based architectures are applied to real-world computer vision, video, and visionâ€“language problems.

The goal of this repo is twofold:
- Serve as a **personal knowledge base** for long-term reference
- Act as a **central index** to larger, production-oriented projects built during this journey

---

## ğŸ§  Topics Covered

- Attention Mechanism
- Vision Transformer (ViT)
- *Ongoing...*

---

## ğŸ—‚ Repository Structure
```
transformers-for-vision/
â”‚
â”œâ”€â”€ notes/ # Handwritten notes (PDF) organized by topic
â”œâ”€â”€ code/ # Concept-level implementations and experiments
â”œâ”€â”€ projects/ # Links to standalone end-to-end project repositories
â”œâ”€â”€ resources/ # Papers, blogs, and reference material
â””â”€â”€ README.md
```

## ğŸ›  Tech Stack

- Python  
- PyTorch  
- Hugging Face Transformers & Diffusers  
- NumPy, Matplotlib  
- Streamlit (for demos)
---

## ğŸ“¬ Contact

If youâ€™d like to discuss any of the work here or collaborate, feel free to reach out via GitHub or LinkedIn.

---

â­ If you find this repository useful, feel free to star it.
